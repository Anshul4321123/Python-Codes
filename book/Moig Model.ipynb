{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a30795-e8b5-4b43-99a6-785354fa7654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, expr, udf, lit, to_timestamp, mean, stddev, max, count, first, min, last, unix_timestamp, trim, collect_list, monotonically_increasing_id, current_timestamp, date_format, lpad, concat, explode, flatten, element_at, array_except, array, array_remove, slice, size, when, collect_set, broadcast, array_intersect, sum, row_number, struct, abs, any_value, to_json\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import IntegerType\n",
    "import builtins as b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e3c140-c95e-43b9-915a-965a9b7ec8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "serialized_storage = StorageLevel(useDisk=True, useMemory=True, useOffHeap=True, deserialized=False, replication=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4077dfa-f1ef-4f28-bb9d-46b3c3135473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "snowflake_options_thres = {\n",
    "    \"sfURL\": \"https://dumzrka-fy75904.snowflakecomputing.com\",\n",
    "    \"sfWarehouse\": \"TAKUMI_AN_01_WH\",\n",
    "    \"sfDatabase\": \"TAKUMI_03\",\n",
    "    \"sfSchema\": \"THRES\",\n",
    "    \"sfRole\": \"DATA_ANALYST\",\n",
    "    \"user\": \"TAKUMI_AN_01\",\n",
    "    \"password\": \"TAKUMI_AN_01@123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36036ef-52b1-4897-a3c1-f82dcd4031b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "snowflake_options = {\n",
    "    \"sfURL\": \"https://dumzrka-fy75904.snowflakecomputing.com\",\n",
    "    \"sfWarehouse\": \"TAKUMI_AN_01_WH\",\n",
    "    \"sfDatabase\": \"TAKUMI_03\",\n",
    "    \"sfSchema\": \"ETL\",\n",
    "    \"sfRole\": \"DATA_ANALYST\",\n",
    "    \"user\": \"TAKUMI_AN_01\",\n",
    "    \"password\": \"TAKUMI_AN_01@123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa82b27-1a02-45ad-a27a-cec55fc86219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "snowflake_options_alert = {\n",
    "    \"sfURL\": \"https://dumzrka-fy75904.snowflakecomputing.com\",\n",
    "    \"sfWarehouse\": \"TAKUMI_AN_01_WH\",\n",
    "    \"sfDatabase\": \"TAKUMI_03\",\n",
    "    \"sfSchema\": \"ALERTS\",\n",
    "    \"sfRole\": \"DATA_ANALYST\",\n",
    "    \"user\": \"TAKUMI_AN_01\",\n",
    "    \"password\": \"TAKUMI_AN_01@123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca295a0d-f0d3-4e34-820d-f6a774f5186f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global start time: 1741858200000000\nGlobal end time: 1741859978000038\nTime to query overall period: 1.9349262714385986 seconds\n"
     ]
    }
   ],
   "source": [
    "time_for_query_session = time.time()\n",
    "overall_query = \"\"\"\n",
    "    SELECT \n",
    "        MIN((DATE_PART('EPOCH_SECOND', TO_TIMESTAMP_NTZ(LEFT(transaction_timestamp, 19))) * 1000000) + CAST(RIGHT(transaction_timestamp, 9) AS BIGINT)) AS min_ts_ns,\n",
    "        MAX((DATE_PART('EPOCH_SECOND', TO_TIMESTAMP_NTZ(LEFT(transaction_timestamp, 19))) * 1000000) + CAST(RIGHT(transaction_timestamp, 9) AS BIGINT)) AS max_ts_ns\n",
    "    FROM  MOIG_TESTING\n",
    "    WHERE (validation_flag IS NULL OR TRIM(validation_flag) = '') ;\n",
    "\"\"\"\n",
    "\n",
    "overall_df = spark.read.format(\"snowflake\") \\\n",
    "    .options(**snowflake_options) \\\n",
    "    .option(\"query\", overall_query) \\\n",
    "    .load()\n",
    "    \n",
    "\n",
    "\n",
    "overall_row = overall_df.collect()[0]\n",
    "global_start_time = overall_row[\"MIN_TS_NS\"]\n",
    "global_end_time = overall_row[\"MAX_TS_NS\"]\n",
    "\n",
    "print(f\"Global start time: {global_start_time}\")\n",
    "print(f\"Global end time: {global_end_time}\")\n",
    "\n",
    "print(f\"Time to query overall period: {time.time() - time_for_query_session} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165e718d-986e-4121-b0f4-57fb189a533e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>MIN_TS_NS</th><th>MAX_TS_NS</th></tr></thead><tbody><tr><td>1741858200000000</td><td>1741859978000038</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1741858200000000",
         "1741859978000038"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "MIN_TS_NS",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "MAX_TS_NS",
         "type": "\"decimal(38,0)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "overall_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13712ddf-5498-498c-b220-556962acd26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000000 120000000 300000000 720000000 1.0 3600000000 1.0 1.0 0.02 0.05 0.6 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "asset_df = spark.read.format(\"snowflake\").options(**snowflake_options_thres).option(\"query\", \"\"\"\n",
    "    SELECT ASSET_ANALYTICS_ID as asset_an_id FROM ASSETANALYTICS WHERE ANALYTICS_ID = 1;\n",
    "\"\"\").load()\n",
    "\n",
    "asset_row = asset_df.collect()[0]\n",
    "asset_analytics_id = asset_row[\"ASSET_AN_ID\"]\n",
    "\n",
    "\n",
    "# Read from Snowflake\n",
    "query = f\"\"\"\n",
    "    SELECT THRESHOLD_ID, THRESHOLD_IDENTIFIER, THRESHOLD_VALUE\n",
    "    FROM AnalyticsThreshold\n",
    "    WHERE asset_analytics_id = {asset_analytics_id}\n",
    "\"\"\"\n",
    "df = spark.read.format(\"snowflake\").options(**snowflake_options_thres).option(\"query\", query).load()\n",
    "\n",
    "threshold_id_map = {}\n",
    "# Collect and assign values dynamically\n",
    "for row in df.collect():\n",
    "    threshold_identifier = row[\"THRESHOLD_IDENTIFIER\"]\n",
    "    threshold_value = row[\"THRESHOLD_VALUE\"]\n",
    "    threshold_id = row[\"THRESHOLD_ID\"]\n",
    "\n",
    "    # Assign to global variable\n",
    "    globals()[threshold_identifier] = threshold_value\n",
    "\n",
    "    # Store ID in a map\n",
    "    threshold_id_map[threshold_identifier] = int(threshold_id)\n",
    "\n",
    "IGNITION_WINDOW = int(IGNITION_WINDOW)\n",
    "FOLLOW_THROUGH = int(FOLLOW_THROUGH)\n",
    "REVERSAL_WINDOW = int(REVERSAL_WINDOW)\n",
    "\n",
    "\n",
    "IGNITION_WINDOW_NS = IGNITION_WINDOW * 60 * 10**6\n",
    "FOLLOW_THROUGH_NS = FOLLOW_THROUGH * 60 * 10**6\n",
    "REVERSAL_WINDOW_NS = REVERSAL_WINDOW * 60 * 10**6\n",
    "TOTAL_WINDOW_NS = IGNITION_WINDOW_NS + FOLLOW_THROUGH_NS + REVERSAL_WINDOW_NS\n",
    "CHUNK_SIZE = 1 * 60 * 60 * 10**6\n",
    "THRES_PRICE_MOIG = MOIG_THRESHOLD_PRICE\n",
    "THRES_QUANTITY_MOIG = MOIG_THRESHOLD_VOLUME\n",
    "THRES_MOMENTUM_MOIG = MOIG_THRESHOLD_MOMENTUM / 100\n",
    "THRES_TRADER_ID_MATCH_PERCENTAGE_REVERSAL = THRES_TRADER_ID_MATCH_PERCENTAGE_REVERSAL / 100\n",
    "THRES_TRADER_ID_MATCH_PERCENTAGE_FOLLOW = THRES_TRADER_ID_MATCH_PERCENTAGE_FOLLOW / 100\n",
    "\n",
    "# Example: print the values to check\n",
    "print(IGNITION_WINDOW_NS, FOLLOW_THROUGH_NS, REVERSAL_WINDOW_NS, TOTAL_WINDOW_NS, PERCENTAGE_ADV30, CHUNK_SIZE, THRES_PRICE_MOIG, THRES_QUANTITY_MOIG, THRES_MOMENTUM_MOIG, THRES_TRADER_ID_MATCH_PERCENTAGE_REVERSAL, THRES_TRADER_ID_MATCH_PERCENTAGE_FOLLOW, THRES_IGNITION_BUY_SELL_RATIO, REVERSAL_THRESHOLD_SELL_BUY_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b6cec7-6231-44aa-8726-0684ca3e8e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>alert_id</th><th>asset_analytics_id</th><th>alert_score</th><th>alert_severity</th><th>alert_status_id</th><th>triggering_transaction_internal_id</th><th>creation_timestamp</th><th>last_update_timestamp</th><th>alert_phases</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "alert_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "asset_analytics_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "alert_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "alert_severity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "alert_status_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "triggering_transaction_internal_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "creation_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "last_update_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "alert_phases",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Frame in consideration: 09:30 to 10:30\nTotal Momentum Ignition Events: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_chunk_start = global_start_time\n",
    "momentum_ignition_total_count = 0\n",
    "next_window =  False  # Flag to indicate subsequent iterations\n",
    "total_time_for_data = 0\n",
    "\n",
    "while (current_chunk_start < global_end_time):\n",
    "    time_to_check_empty = time.time()\n",
    "    if current_chunk_start == global_start_time:\n",
    "        current_chunk_end = current_chunk_start + CHUNK_SIZE # 1 hour in nanoseconds\n",
    "    else:\n",
    "        current_chunk_end = current_chunk_start + CHUNK_SIZE + TOTAL_WINDOW_NS\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT transaction_id, transaction_timestamp, symbol, price, quantity, validation_flag, trader_id, side_id, adv30\n",
    "        FROM MOIG_TESTING\n",
    "        WHERE (validation_flag IS NULL OR TRIM(validation_flag) = '')\n",
    "        AND ((DATE_PART('EPOCH_SECOND', TO_TIMESTAMP_NTZ(LEFT(transaction_timestamp, 19))) * 1000000) +\n",
    "                CAST(RIGHT(transaction_timestamp, 6) AS BIGINT))\n",
    "            BETWEEN {current_chunk_start} AND {current_chunk_end}\n",
    "        ORDER BY symbol, transaction_timestamp;\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_df = spark.read.format(\"snowflake\") \\\n",
    "        .options(**snowflake_options) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    chunk_df_count = chunk_df.count()\n",
    "    \n",
    "\n",
    "    chunk_df = chunk_df.filter(col(\"quantity\") > (col(\"adv30\") * (PERCENTAGE_ADV30/10000000000)))\n",
    "    \n",
    "    if chunk_df_count <= 0:\n",
    "        current_chunk_start += CHUNK_SIZE + TOTAL_WINDOW_NS\n",
    "        continue\n",
    "    total_time_to_check_empty = time.time() - time_to_check_empty\n",
    "    \n",
    "    \n",
    "    estimated_size_mb = (chunk_df_count * 120) / (1024 ** 2)\n",
    "    shuffle_partition_count = math.ceil(estimated_size_mb / 128)\n",
    "\n",
    "    spark_cores = spark.sparkContext.defaultParallelism\n",
    "    shuffle_partitions = b.max(shuffle_partition_count, spark_cores)\n",
    "\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "\n",
    "    chunk_df = chunk_df.withColumn(\"trigger_trx_ts_epoch\",\n",
    "        (expr(\"CAST(UNIX_TIMESTAMP(LEFT(transaction_timestamp, 19)) AS BIGINT) * 1000000\") +\n",
    "        expr(\"CAST(RIGHT(transaction_timestamp, 6) AS BIGINT)\"))\n",
    "    )\n",
    "    \n",
    "    if next_window:\n",
    "        skip_overlap_start = current_chunk_start + TOTAL_WINDOW_NS\n",
    "        chunk_df = chunk_df.withColumn(\"is_overlap\",\n",
    "            when(col(\"trigger_trx_ts_epoch\") < lit(skip_overlap_start), lit(True)).otherwise(lit(False))\n",
    "        )\n",
    "    else:\n",
    "        next_window = True\n",
    "        chunk_df = chunk_df.withColumn(\"is_overlap\", lit(False))\n",
    "\n",
    "    chunk_df = chunk_df.withColumnRenamed( \"transaction_id\", \"trigger_trx_id\") \\\n",
    "                .withColumnRenamed(\"symbol\", \"trigger_symbol\") \\\n",
    "                .withColumnRenamed( \"transaction_timestamp\", \"trigger_trx_ts\")          \n",
    "    \n",
    "    \n",
    "\n",
    "    chunk_df = chunk_df.withColumn(\n",
    "        \"buy_volume\", when(col(\"side_id\") == 1, col(\"quantity\")).otherwise(0).cast(\"int\")\n",
    "    ).withColumn(\n",
    "        \"sell_volume\", when(col(\"side_id\") == 2, col(\"quantity\")).otherwise(0).cast(\"int\")\n",
    "    )\n",
    "    \n",
    "    chunk_df.persist(serialized_storage)\n",
    "    \n",
    "    ignition_window = Window.partitionBy(\"trigger_symbol\") \\\n",
    "                        .orderBy(col(\"trigger_trx_ts_epoch\").cast(\"long\")) \\\n",
    "                        .rangeBetween(-IGNITION_WINDOW_NS - FOLLOW_THROUGH_NS - REVERSAL_WINDOW_NS, -FOLLOW_THROUGH_NS - REVERSAL_WINDOW_NS)\n",
    "    \n",
    "    follow_through_window = Window.partitionBy(\"trigger_symbol\") \\\n",
    "                        .orderBy(col(\"trigger_trx_ts_epoch\").cast(\"long\")) \\\n",
    "                        .rangeBetween(-FOLLOW_THROUGH_NS - REVERSAL_WINDOW_NS, - REVERSAL_WINDOW_NS)\n",
    "    \n",
    "    reversal_window = Window.partitionBy(\"trigger_symbol\") \\\n",
    "                        .orderBy(col(\"trigger_trx_ts_epoch\").cast(\"long\")) \\\n",
    "                        .rangeBetween(-REVERSAL_WINDOW_NS, 0)\n",
    "\n",
    "    ignition_expressions = {\n",
    "        \"iw_avg_price\": mean(\"price\").over(ignition_window),\n",
    "        \"iw_std_price\": stddev(\"price\").over(ignition_window),\n",
    "        \"iw_avg_volume\": mean(\"quantity\").over(ignition_window),\n",
    "        \"iw_std_volume\": stddev(\"quantity\").over(ignition_window),\n",
    "        \"iw_momentum\": (last(\"price\").over(ignition_window) - first(\"price\").over(ignition_window)) / first(\"price\").over(ignition_window),\n",
    "        \"iw_last_price\": last(\"price\").over(ignition_window),\n",
    "        \"iw_last_volume\": last(\"quantity\").over(ignition_window),\n",
    "        \"iw_max_price\": max(\"price\").over(ignition_window),\n",
    "        \"iw_f_trx\": first(\"trigger_trx_ts\").over(ignition_window),\n",
    "        \"iw_l_trx\": last(\"trigger_trx_ts\").over(ignition_window),\n",
    "        \"iw_price_th\": col(\"iw_avg_price\") + THRES_PRICE_MOIG * col(\"iw_std_price\"),\n",
    "        \"iw_volume_th\": col(\"iw_avg_volume\") + THRES_QUANTITY_MOIG * col(\"iw_std_volume\"),\n",
    "        \"iw_total_buy_volume\": sum(col(\"buy_volume\").cast(\"int\")).over(ignition_window),\n",
    "        \"iw_total_sell_volume\": sum(col(\"sell_volume\").cast(\"int\")).over(ignition_window),\n",
    "        \"buy_sell_ratio\": (col(\"iw_total_buy_volume\") / col(\"iw_total_sell_volume\")).cast(\"float\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    stats_df = chunk_df.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        *[\n",
    "            when(col(\"is_overlap\") == False, expr).alias(col_name)\n",
    "            for col_name, expr in ignition_expressions.items()\n",
    "        ]\n",
    "    )\n",
    "    stats_df.persist(serialized_storage)\n",
    "    chunk_df.unpersist(blocking=True)\n",
    "\n",
    "    ignition_condition = (\n",
    "        (col(\"iw_last_price\") > col(\"iw_price_th\")) &\n",
    "        (col(\"iw_last_volume\") > col(\"iw_volume_th\")) &\n",
    "        (col(\"iw_momentum\") > THRES_MOMENTUM_MOIG) &\n",
    "        (\n",
    "            ((col(\"iw_total_buy_volume\") > 0) & (col(\"iw_total_sell_volume\") > 0) & (col(\"buy_sell_ratio\") > THRES_IGNITION_BUY_SELL_RATIO)) |\n",
    "            ((col(\"iw_total_buy_volume\") > 0) & (col(\"iw_total_sell_volume\") == 0))\n",
    "        ) &\n",
    "        (~col(\"is_overlap\"))\n",
    "    )\n",
    "\n",
    "    stats_df = stats_df.withColumn(\"phase\", when(ignition_condition, lit(\"Ignition\"))) \\\n",
    "        .withColumn(\"price_breach_pct\", when(ignition_condition, ((col(\"iw_last_price\") - col(\"iw_price_th\")) / col(\"iw_price_th\")) * 100)) \\\n",
    "        .withColumn(\"volume_breach_pct\", when(ignition_condition, ((col(\"iw_last_volume\") - col(\"iw_volume_th\")) / col(\"iw_volume_th\")) * 100)) \\\n",
    "        .withColumn(\"momentum_breach_pct\", when(ignition_condition, abs((col(\"iw_momentum\") - (lit(THRES_MOMENTUM_MOIG) * 100)) / (lit(THRES_MOMENTUM_MOIG) * 100) * 100))) \\\n",
    "        .withColumn(\"buy_sell_ratio_breach_pct\", when(ignition_condition, col(\"buy_sell_ratio\"))\n",
    "    ).select(\n",
    "        \"trigger_symbol\", \"trigger_trx_ts\", \"trigger_trx_id\", \"iw_avg_price\", \"iw_f_trx\", \"iw_l_trx\", \"trader_id\",\"price\",\n",
    "        \"iw_max_price\", \"trigger_trx_ts_epoch\",\"price_breach_pct\", \"volume_breach_pct\", \"momentum_breach_pct\", \"buy_sell_ratio_breach_pct\", \"phase\", \"buy_volume\",\"sell_volume\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # print(\"Count after aggregations: \",stats_df.count())\n",
    "\n",
    "    ignition_symbols_df = stats_df.filter(col(\"phase\") == \"Ignition\") \\\n",
    "                              .select(\"trigger_symbol\") \\\n",
    "                              .distinct()\n",
    "    \n",
    "    stats_df = stats_df.join(ignition_symbols_df, on=\"trigger_symbol\", how=\"inner\")\n",
    "    # print(\"After the filter of distinct Symbols: \",stats_df.count())\n",
    "    \n",
    "    stats_df = stats_df.withColumn(\"iw_trx_range\", when(col(\"phase\") == \"Ignition\", collect_list(\"trigger_trx_id\").over(ignition_window))\n",
    "    ).withColumn(\"iw_trader_ids\", when(col(\"phase\") == \"Ignition\", collect_set(\"trader_id\").over(ignition_window)))\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    ignition_df = stats_df.filter(col(\"phase\") == \"Ignition\") \\\n",
    "        .select(\"trigger_symbol\", \"trigger_trx_ts_epoch\")\n",
    "\n",
    "    ignition_df = ignition_df.withColumn(\"start_epoch\", col(\"trigger_trx_ts_epoch\") - (FOLLOW_THROUGH_NS + REVERSAL_WINDOW_NS)) \\\n",
    "                            .withColumnRenamed(\"trigger_trx_ts_epoch\", \"ignition_epoch\")\n",
    "    \n",
    "    joined_df = stats_df.alias(\"s\").join(\n",
    "        ignition_df.alias(\"i\"),\n",
    "        (col(\"s.trigger_symbol\") == col(\"i.trigger_symbol\")) &\n",
    "        (col(\"s.trigger_trx_ts_epoch\") >= col(\"i.start_epoch\")) &\n",
    "        (col(\"s.trigger_trx_ts_epoch\") <= col(\"i.ignition_epoch\"))\n",
    "    ).select(\"s.*\")  # Select only original stats_df columns\n",
    "   \n",
    "    # print(joined_df.count())\n",
    "    # Optional: Remove duplicates if needed\n",
    "    joined_df = joined_df.dropDuplicates()\n",
    "    joined_df.persist(serialized_storage)\n",
    "    stats_df.unpersist(blocking=True)\n",
    "    stats_df = joined_df \n",
    "    \n",
    "    # break\n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"final stats_df\" , stats_df.count())\n",
    "\n",
    "    follow_through_expressions = {\n",
    "        \"ftw_max_price\": max(\"price\").over(follow_through_window),\n",
    "        \"fw_trx_range\" : collect_list(\"trigger_trx_id\").over(follow_through_window),\n",
    "        \"fw_trader_ids\" : collect_set(\"trader_id\").over(follow_through_window),\n",
    "        \"trader_id_match_count\" : size(array_intersect(col(\"fw_trader_ids\"), col(\"iw_trader_ids\"))),\n",
    "        \"iw_trader_id_count\" : size(col(\"iw_trader_ids\")),\n",
    "        \"fw_trader_id_match_percentage\" : col(\"trader_id_match_count\") / col(\"iw_trader_id_count\")\n",
    "    }\n",
    "\n",
    "    stats_df = stats_df.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        *[\n",
    "            when(col(\"phase\") == \"Ignition\", expr).alias(col_name)\n",
    "            for col_name, expr in follow_through_expressions.items()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # stats_df.display()\n",
    "    # print(stats_df.count())\n",
    "    \n",
    "    follow_through_condition = (\n",
    "        (col(\"phase\") == \"Ignition\") &\n",
    "        (col(\"ftw_max_price\") > col(\"iw_max_price\")) &\n",
    "        (col(\"fw_trader_id_match_percentage\") <= lit(THRES_TRADER_ID_MATCH_PERCENTAGE_FOLLOW))\n",
    "    )\n",
    "\n",
    "    # Step 4: Add Follow-Through phase flag and mismatch percentage column\n",
    "    stats_df = stats_df.withColumn(\"is_follow_through\", when(follow_through_condition, lit(\"Follow-Through\"))) \\\n",
    "                .withColumn(\"trader_id_mismatch_pct_follow\", \n",
    "                            when(follow_through_condition, col(\"fw_trader_id_match_percentage\") * 100)\n",
    "                            ).select(\n",
    "                                \"trigger_symbol\",\"trigger_trx_ts\",\"trigger_trx_id\",\"iw_avg_price\",\"trader_id\",\"price\",\"trigger_trx_ts_epoch\",\"price_breach_pct\",\"volume_breach_pct\",\"momentum_breach_pct\",\"buy_sell_ratio_breach_pct\",\"phase\",\"buy_volume\",\"sell_volume\",\"iw_trx_range\",\"iw_trader_ids\",\"fw_trx_range\",\"iw_trader_id_count\",\"is_follow_through\",\"trader_id_mismatch_pct_follow\")\n",
    "    \n",
    "    # print(stats_df.count())\n",
    "\n",
    "    follow_through_df = stats_df.filter((col(\"phase\") == \"Ignition\") & (col(\"is_follow_through\") == \"Follow-Through\")) \\\n",
    "        .select(\"trigger_symbol\", \"trigger_trx_ts_epoch\")\n",
    "\n",
    "    follow_through_df = follow_through_df.withColumn(\"start_epoch\", col(\"trigger_trx_ts_epoch\") - (REVERSAL_WINDOW_NS)) \\\n",
    "                            .withColumnRenamed(\"trigger_trx_ts_epoch\", \"ignition_epoch\")\n",
    "\n",
    "    joined_df = stats_df.alias(\"s\").join(\n",
    "        follow_through_df.alias(\"i\"),\n",
    "        (col(\"s.trigger_symbol\") == col(\"i.trigger_symbol\")) &\n",
    "        (col(\"s.trigger_trx_ts_epoch\") >= col(\"i.start_epoch\")) &\n",
    "        (col(\"s.trigger_trx_ts_epoch\") <= col(\"i.ignition_epoch\"))\n",
    "    ).select(\"s.*\")\n",
    "\n",
    "    joined_df = joined_df.dropDuplicates()\n",
    "    joined_df.persist(serialized_storage)\n",
    "    stats_df.unpersist(blocking=True)\n",
    "    stats_df = joined_df \n",
    "    \n",
    "    reversal_expressions = {\n",
    "        \"rw_min_price\": min(\"price\").over(reversal_window),\n",
    "        \"rw_trx_range\" : collect_list(\"trigger_trx_id\").over(reversal_window),\n",
    "        \"rw_trader_ids\" : collect_set(\"trader_id\").over(reversal_window),\n",
    "        \"trader_id_match_count\" : size(array_intersect(col(\"rw_trader_ids\"), col(\"iw_trader_ids\"))),\n",
    "        \"iw_trader_id_count\" : size(col(\"iw_trader_ids\")),\n",
    "        \"rw_trader_id_match_percentage\" : col(\"trader_id_match_count\") / col(\"iw_trader_id_count\"),\n",
    "        \"rw_total_buy_volume\" : sum(col(\"buy_volume\")).over(reversal_window),\n",
    "        \"rw_total_sell_volume\" : sum(col(\"sell_volume\")).over(reversal_window),\n",
    "        \"sell_buy_ratio\" : (col(\"rw_total_sell_volume\") / col(\"rw_total_buy_volume\"))\n",
    "    }\n",
    "\n",
    "    stats_df = stats_df.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        *[\n",
    "            when((col(\"phase\") == \"Ignition\") & (col(\"is_follow_through\") == \"Follow-Through\"), expr).alias(col_name)\n",
    "            for col_name, expr in reversal_expressions.items()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    reversal_condition = (\n",
    "        (col(\"phase\") == \"Ignition\") &\n",
    "        (col(\"is_follow_through\") == \"Follow-Through\") &\n",
    "        (col(\"rw_min_price\") < col(\"iw_avg_price\")) &\n",
    "        (col(\"rw_trader_id_match_percentage\") >= lit(THRES_TRADER_ID_MATCH_PERCENTAGE_REVERSAL)) &  # later for this there will be the threshold\n",
    "        (\n",
    "            ((col(\"rw_total_buy_volume\") > 0) & (col(\"rw_total_sell_volume\") > 0) & (col(\"sell_buy_ratio\") > REVERSAL_THRESHOLD_SELL_BUY_RATIO)) |\n",
    "            ((col(\"rw_total_sell_volume\") > 0) & (col(\"rw_total_buy_volume\") == 0))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    momentum_ignition_df = stats_df.filter(reversal_condition).withColumn(\n",
    "                            \"all_trx_range\",\n",
    "                            concat(col(\"iw_trx_range\"), col(\"fw_trx_range\"), col(\"rw_trx_range\"))\n",
    "                        ).select(\n",
    "                            \"trigger_symbol\",\"trigger_trx_ts\",\"trigger_trx_id\",\"trigger_trx_ts_epoch\",\"price_breach_pct\",\"volume_breach_pct\",\"momentum_breach_pct\",\"buy_sell_ratio_breach_pct\",\"all_trx_range\",\"trader_id_mismatch_pct_follow\",\n",
    "                            (col(\"rw_trader_id_match_percentage\") * 100).alias(\"trader_id_match_pct_reversal\"),\n",
    "                            col(\"sell_buy_ratio\").alias(\"sell_buy_ratio_breach_pct\"),\n",
    "                            lit(\"Momentum-Ignition\").alias(\"isMomentum\")\n",
    "                        )\n",
    "\n",
    "    momentum_ignition_df.persist(serialized_storage)\n",
    "    momentum_ignition_total_count += momentum_ignition_df.count()\n",
    "    \n",
    "    # Step 1: Join all three phases on trigger_trx_id\n",
    "    breach_df = momentum_ignition_df.select(\n",
    "        col(\"trigger_trx_ts\"),\n",
    "        element_at(col(\"all_trx_range\"), -1).alias(\"triggering_trx\"),\n",
    "        col(\"price_breach_pct\"),\n",
    "        col(\"volume_breach_pct\"),\n",
    "        col(\"momentum_breach_pct\"),\n",
    "        col(\"buy_sell_ratio_breach_pct\"),\n",
    "        col(\"trader_id_mismatch_pct_follow\"),\n",
    "        col(\"trader_id_match_pct_reversal\"),\n",
    "        col(\"sell_buy_ratio_breach_pct\")\n",
    "    )\n",
    "\n",
    "    window_spec = Window.orderBy(\"trigger_trx_ts\")\n",
    "\n",
    "    alert_query = \"\"\"\n",
    "        (\n",
    "            SELECT ALERT_ID\n",
    "            FROM THRESHOLD_BREACH\n",
    "            WHERE ALERT_ID LIKE 'MOIG_%'\n",
    "            ORDER BY TRY_TO_NUMBER(SUBSTRING(ALERT_ID, 6)) DESC\n",
    "            LIMIT 1\n",
    "        ) AS last_alert\n",
    "    \"\"\"\n",
    "\n",
    "    last_alert_id_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_options_alert) \\\n",
    "        .option(\"query\", alert_query) \\\n",
    "        .load()\n",
    "    \n",
    "    last_id_row = last_alert_id_df.collect()\n",
    "    if last_id_row:\n",
    "        last_alert_id_str = last_id_row[0][\"ALERT_ID\"]\n",
    "        last_num = int(last_alert_id_str.replace(\"MOIG_\", \"\"))\n",
    "    else:\n",
    "        last_num = 0\n",
    "    \n",
    "    offset = last_num\n",
    "\n",
    "    # Generate alert metadata\n",
    "    alert_df = momentum_ignition_df \\\n",
    "        .withColumn(\"incremental_id\", row_number().over(window_spec)) \\\n",
    "        .withColumn(\"generated_id\", col(\"incremental_id\") + offset) \\\n",
    "        .withColumn(\"alert_id\", concat(lit(\"MOIG_\"), col(\"generated_id\"))) \\\n",
    "        .withColumn(\"triggering_transaction_internal_id\", element_at(col(\"all_trx_range\"), -1)) \\\n",
    "        .withColumn(\"trigger_trx_ts_ts\", col(\"trigger_trx_ts\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\n",
    "        \"alert_phases\",\n",
    "        to_json(\n",
    "            array(\n",
    "                struct(\n",
    "                    lit(\"Ignition Phase\").alias(\"phase_name\"),\n",
    "                    (col(\"trigger_trx_ts_ts\") - expr(f\"INTERVAL {IGNITION_WINDOW + FOLLOW_THROUGH + REVERSAL_WINDOW} MINUTES\")).alias(\"start_date\"),\n",
    "                    (col(\"trigger_trx_ts_ts\") - expr(f\"INTERVAL {FOLLOW_THROUGH + REVERSAL_WINDOW} MINUTES\")).alias(\"end_date\")\n",
    "                ),\n",
    "                struct(\n",
    "                    lit(\"Follow-Through Phase\").alias(\"phase_name\"),\n",
    "                    (col(\"trigger_trx_ts_ts\") - expr(f\"INTERVAL {FOLLOW_THROUGH + REVERSAL_WINDOW} MINUTES\")).alias(\"start_date\"),\n",
    "                    (col(\"trigger_trx_ts_ts\") - expr(f\"INTERVAL {REVERSAL_WINDOW} MINUTES\")).alias(\"end_date\")\n",
    "                ),\n",
    "                struct(\n",
    "                    lit(\"Reversal Phase\").alias(\"phase_name\"),\n",
    "                    (col(\"trigger_trx_ts_ts\") - expr(f\"INTERVAL {REVERSAL_WINDOW} MINUTES\")).alias(\"start_date\"),\n",
    "                    col(\"trigger_trx_ts_ts\").alias(\"end_date\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) \\\n",
    "        .filter(col(\"triggering_transaction_internal_id\").isNotNull()) \\\n",
    "        .select(\n",
    "            \"alert_id\", \"trigger_trx_ts\",\n",
    "            lit(1).alias(\"asset_analytics_id\"),\n",
    "            lit(0.9).alias(\"alert_score\"),\n",
    "            lit(\"High\").alias(\"alert_severity\"),\n",
    "            lit(1).alias(\"alert_status_id\"),\n",
    "            \"triggering_transaction_internal_id\",\n",
    "            current_timestamp().alias(\"creation_timestamp\"),\n",
    "            current_timestamp().alias(\"last_update_timestamp\"),\n",
    "            \"all_trx_range\",\n",
    "            \"alert_phases\"\n",
    "    )\n",
    "    \n",
    "    joined_df = breach_df.alias(\"b\").join(\n",
    "        alert_df.alias(\"a\"),\n",
    "        col(\"b.triggering_trx\") == col(\"a.triggering_transaction_internal_id\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    # Map DataFrame columns to threshold IDs\n",
    "    column_to_threshold = {\n",
    "        'price_breach_pct': threshold_id_map['MOIG_THRESHOLD_PRICE'],\n",
    "        'volume_breach_pct': threshold_id_map['MOIG_THRESHOLD_VOLUME'],\n",
    "        'momentum_breach_pct': threshold_id_map['MOIG_THRESHOLD_MOMENTUM'],\n",
    "        'buy_sell_ratio_breach_pct': threshold_id_map['THRES_IGNITION_BUY_SELL_RATIO'],\n",
    "        'trader_id_mismatch_pct_follow': threshold_id_map['THRES_TRADER_ID_MATCH_PERCENTAGE_FOLLOW'],\n",
    "        'trader_id_match_pct_reversal': threshold_id_map['THRES_TRADER_ID_MATCH_PERCENTAGE_REVERSAL'],\n",
    "        'sell_buy_ratio_breach_pct': threshold_id_map['REVERSAL_THRESHOLD_SELL_BUY_RATIO'],\n",
    "        'ignition_window': threshold_id_map['IGNITION_WINDOW'],\n",
    "        'reversal_window': threshold_id_map['FOLLOW_THROUGH'],\n",
    "        'reversal_window': threshold_id_map['REVERSAL_WINDOW'],\n",
    "        'ADV30_breach' : threshold_id_map['PERCENTAGE_ADV30']\n",
    "    }\n",
    "    \n",
    "    threshold_mapping = [\n",
    "        (metric, threshold_id) for metric, threshold_id in column_to_threshold.items()\n",
    "    ]\n",
    "    threshold_df = spark.createDataFrame(threshold_mapping, [\"metric\", \"threshold_id\"])\n",
    "    \n",
    "\n",
    "    alert_breach_df = joined_df.select(\n",
    "        col(\"a.alert_id\"),\n",
    "        array(\n",
    "            struct(lit(\"price_breach_pct\").alias(\"metric\"), col(\"b.price_breach_pct\").alias(\"value\")),\n",
    "            struct(lit(\"volume_breach_pct\").alias(\"metric\"), col(\"b.volume_breach_pct\").alias(\"value\")),\n",
    "            struct(lit(\"momentum_breach_pct\").alias(\"metric\"), col(\"b.momentum_breach_pct\").alias(\"value\")),\n",
    "            struct(lit(\"buy_sell_ratio_breach_pct\").alias(\"metric\"), col(\"b.buy_sell_ratio_breach_pct\").alias(\"value\")),\n",
    "            struct(lit(\"trader_id_mismatch_pct_follow\").alias(\"metric\"), col(\"b.trader_id_mismatch_pct_follow\").alias(\"value\")),\n",
    "            struct(lit(\"trader_id_match_pct_reversal\").alias(\"metric\"), col(\"b.trader_id_match_pct_reversal\").alias(\"value\")),\n",
    "            struct(lit(\"sell_buy_ratio_breach_pct\").alias(\"metric\"), col(\"b.sell_buy_ratio_breach_pct\").alias(\"value\")),\n",
    "            struct(lit(\"ignition_window\").alias(\"metric\"), lit(None).cast(\"string\").alias(\"value\")),\n",
    "            struct(lit(\"reversal_window\").alias(\"metric\"), lit(None).cast(\"string\").alias(\"value\")),\n",
    "            struct(lit(\"reversal_window\").alias(\"metric\"), lit(None).cast(\"string\").alias(\"value\")),\n",
    "            struct(lit(\"ADV30_breach\").alias(\"metric\"), lit(None).cast(\"string\").alias(\"value\"))\n",
    "        ).alias(\"breach_metrics\")\n",
    "    )\n",
    "    \n",
    "    exploded_df = alert_breach_df.select(\n",
    "        col(\"alert_id\"),\n",
    "        explode(col(\"breach_metrics\")).alias(\"metric_struct\")\n",
    "    ).select(\n",
    "        col(\"alert_id\"),\n",
    "        col(\"metric_struct.metric\").alias(\"metric\"),\n",
    "        col(\"metric_struct.value\").alias(\"breach_value\")\n",
    "    )\n",
    "\n",
    "    final_df = exploded_df.join(\n",
    "        threshold_df,\n",
    "        exploded_df.metric == threshold_df.metric,\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        col(\"alert_id\"),\n",
    "        col(\"threshold_id\"),\n",
    "        col(\"breach_value\")\n",
    "    )\n",
    "\n",
    "    # Create alerted_df by excluding the triggering transaction\n",
    "    alerted_df = alert_df.withColumn(\n",
    "        \"remaining_transactions\",\n",
    "        slice(col(\"all_trx_range\"), 1, size(col(\"all_trx_range\")))  # Take all except the last\n",
    "    ).withColumn(\"transaction_internal_id\", explode(col(\"remaining_transactions\"))) \\\n",
    "    .select(\"alert_id\", \"transaction_internal_id\", \"creation_timestamp\", \"last_update_timestamp\")\n",
    "\n",
    "    # Remove all_trx_range from alert_df as it's no longer needed\n",
    "    alert_df = alert_df.drop(\"all_trx_range\")\n",
    "\n",
    "    alert_df = alert_df.select(\n",
    "        \"alert_id\",\n",
    "        \"asset_analytics_id\",\n",
    "        \"alert_score\",\n",
    "        \"alert_severity\",\n",
    "        \"alert_status_id\",\n",
    "        \"triggering_transaction_internal_id\",\n",
    "        \"creation_timestamp\",\n",
    "        \"last_update_timestamp\",\n",
    "        \"alert_phases\"\n",
    "    )\n",
    "\n",
    "    alert_df.persist(serialized_storage)\n",
    "    alert_df.display()\n",
    "    alert_df.write.format(\"snowflake\") \\\n",
    "    .options(**snowflake_options_alert) \\\n",
    "    .option(\"dbtable\", \"ALERT\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "    alerted_df.write.format(\"snowflake\") \\\n",
    "    .options(**snowflake_options_alert) \\\n",
    "    .option(\"dbtable\", \"ALERTED_TRANSACTION\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "    final_df.write.format(\"snowflake\") \\\n",
    "    .options(**snowflake_options_alert) \\\n",
    "    .option(\"dbtable\", \"THRESHOLD_BREACH\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "    chunk_df.unpersist()\n",
    "    stats_df.unpersist()\n",
    "    # ignition_df.unpersist()\n",
    "    # follow_through_df.unpersist()\n",
    "    # reversal_df.unpersist()\n",
    "    alert_df.unpersist()\n",
    "\n",
    "    print(f\"Time Frame in consideration: {datetime.fromtimestamp(int(current_chunk_start) // 10 ** 6).strftime('%H:%M')} to {datetime.fromtimestamp(int(current_chunk_end) // 10 ** 6).strftime('%H:%M')}\")\n",
    "\n",
    "    current_chunk_start = current_chunk_end - TOTAL_WINDOW_NS\n",
    "\n",
    "print(f\"Total Momentum Ignition Events: {momentum_ignition_total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17fb69d-bf5b-4c0e-9ade-4d6bcea576fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time MOIG: 38.003679 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time MOIG: {execution_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Moig Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}